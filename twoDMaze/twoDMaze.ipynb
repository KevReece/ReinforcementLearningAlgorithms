{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.table = [[0] * n_actions for _ in range(n_states)]\n",
    "\n",
    "    def get_greedy_action_policy(self, state):\n",
    "        action_values = self.table[state]\n",
    "        max_q_actions = [\n",
    "            action \n",
    "            for action, q in enumerate(action_values) \n",
    "            if q == max(action_values)\n",
    "        ]\n",
    "        return [\n",
    "            1/len(max_q_actions) if action in max_q_actions else 0 \n",
    "            for action in range(len(action_values))\n",
    "        ]\n",
    "    \n",
    "    def print(self):\n",
    "        for state, actions in enumerate(self.table):\n",
    "            print('State', state, '->', actions)\n",
    "\n",
    "class StateValueTable:\n",
    "    def __init__(self, n_states):\n",
    "        self.table = [0] * n_states\n",
    "\n",
    "    def get_greedy_action_policy(self, action_to_next_state_dynamics, exit_dynamics):\n",
    "        exit_actions = [action for action, next_state in enumerate(action_to_next_state_dynamics) if next_state in exit_dynamics]\n",
    "        greedy_actions = exit_actions\n",
    "        if not greedy_actions:\n",
    "            actions_next_state_values = [\n",
    "                self.table[next_state] if next_state is not None else float('-inf') \n",
    "                for next_state in action_to_next_state_dynamics\n",
    "            ]\n",
    "            max_next_state_value = max([next_state_value for next_state_value in actions_next_state_values])\n",
    "            max_actions = [action for action, next_state_value in enumerate(actions_next_state_values) if next_state_value == max_next_state_value]\n",
    "            greedy_actions = max_actions\n",
    "        return [\n",
    "            1/len(greedy_actions) if action in greedy_actions else 0 \n",
    "            for action in range(len(action_to_next_state_dynamics))\n",
    "        ]\n",
    "\n",
    "    def print(self):\n",
    "        print(self.table)\n",
    "\n",
    "class PolicyFunctions:\n",
    "    @staticmethod\n",
    "    def combine_policy_with_exploratory_policy(action_policy, exploration_rate):\n",
    "        exploratory_action_policy_part = [exploration_rate/len(action_policy)] * len(action_policy)\n",
    "        action_policy_part = [action_policy[action] * (1 - exploration_rate) for action in range(len(action_policy))]\n",
    "        return [exploratory_action_policy_part[action] + action_policy_part[action] for action in range(len(action_policy))]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_actions(state_action_value_function):\n",
    "        max_q = max(state_action_value_function)\n",
    "        max_actions = [action for action, q in enumerate(state_action_value_function) if q == max_q]\n",
    "        return max_actions\n",
    "    \n",
    "    @staticmethod\n",
    "    def single_action_as_action_probabilities(chosen_action, n_actions):\n",
    "        return [\n",
    "            1 if action == chosen_action else 0 \n",
    "            for action in range(n_actions)\n",
    "        ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AbstractAgent import AbstractAgent\n",
    "from Runner import Runner\n",
    "from AbstractEnvironment import AbstractEnvironment\n",
    "from TwoDMazeEnvironment import TwoDMazeEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent: AbstractAgent, environment: AbstractEnvironment):\n",
    "    environment.reset()\n",
    "    runner = Runner(agent, max_steps_per_episode=200)\n",
    "    runner.run(environment, n_episodes=1000, learn=True)\n",
    "    runner.run(environment, n_episodes=1, verbose=True, learn=False)\n",
    "    \n",
    "# Single goal\n",
    "environment = TwoDMazeEnvironment(position_shape=(5,5), initial_agent_position=(0,0), exit_positions=[(4,4)], initial_position_rewards={(4,4): 10})\n",
    "    \n",
    "# Duel goal\n",
    "# environment = TwoDMazeEnvironment(position_shape=(10,10), initial_agent_position=(3,3), exit_positions=[(0,0),(9,9)], initial_position_rewards={(0,0):1, (9,9): 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Step: 1\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 8\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 9\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A|E]\n",
      "Step: 10\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n"
     ]
    }
   ],
   "source": [
    "class QLearningAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.1, discount_factor=0.999, exploration_rate=0.2):\n",
    "        self.q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        greedy_action_policy = self.q_table.get_greedy_action_policy(state)\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        if state == next_state:\n",
    "            return\n",
    "        max_next_q = max(self.q_table.table[next_state])\n",
    "        self.q_table.table[state][action] += self.learning_rate * (reward + self.discount_factor * max_next_q - self.q_table.table[state][action])\n",
    "\n",
    "    def print(self):\n",
    "        self.q_table.print()\n",
    "\n",
    "test_agent(QLearningAgent(environment), environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Step: 1\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 8\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 9\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 10\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 11\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 12\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 13\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 14\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 15\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 16\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 17\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 18\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 19\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 20\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 21\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 22\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 23\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 24\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 25\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 26\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 27\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 28\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 29\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 30\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 31\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 32\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 33\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 34\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 35\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 36\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 37\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 38\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 39\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 40\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 41\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 42\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 43\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 44\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 45\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 46\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 47\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 48\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 49\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 50\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 51\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | |E]\n",
      "Step: 52\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 53\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | |E]\n",
      "Step: 54\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 55\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | |E]\n",
      "Step: 56\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 57\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 58\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 59\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 60\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 61\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 62\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 63\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 64\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 65\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 66\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 67\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 68\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 69\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 70\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 71\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 72\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 73\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 74\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 75\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 76\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 77\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 78\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 79\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 80\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 81\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 82\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 83\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 84\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 85\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 86\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 87\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 88\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 89\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 90\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 91\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 92\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 93\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 94\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 95\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 96\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 97\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 98\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 99\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A|E]\n",
      "Step: 100\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| |E]\n",
      "Step: 101\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A|E]\n",
      "Step: 102\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class DoubleQLearning(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.1, discount_factor=0.999, exploration_rate=0.2):\n",
    "        self.q_table1 = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.q_table2 = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        greedy_action_policy = [\n",
    "            (q1 + q2) / 2 \n",
    "            for q1, q2 in zip(self.q_table1.table[state], self.q_table2.table[state])\n",
    "        ]\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        if state == next_state:\n",
    "            return\n",
    "        if random.choice([True, False]):\n",
    "            q_table = self.q_table1.table\n",
    "            source_q_table = self.q_table2.table\n",
    "        else:\n",
    "            q_table = self.q_table2.table\n",
    "            source_q_table = self.q_table1.table\n",
    "        max_action = q_table[next_state].index(max(q_table[next_state]))\n",
    "        q_table[state][action] += self.learning_rate * (reward + self.discount_factor * source_q_table[next_state][max_action] - q_table[state][action])\n",
    "\n",
    "    def print(self):\n",
    "        self.q_table1.print()\n",
    "        self.q_table2.print()\n",
    "\n",
    "test_agent(DoubleQLearning(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Step: 1\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 8\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 9\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 10\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 11\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 12\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 13\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 14\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 15\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 16\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 17\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 18\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 19\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 20\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 21\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 22\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 23\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 24\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 25\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 26\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 27\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 28\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 29\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 30\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 31\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 32\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 33\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 34\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 35\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 36\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | |E]\n",
      "Step: 37\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | |E]\n",
      "Step: 38\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 39\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 40\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 41\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 42\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 43\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 44\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 45\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 46\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 47\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 48\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 49\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 50\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| |E]\n",
      "Step: 51\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| |E]\n",
      "Step: 52\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 53\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 54\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 55\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 56\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 57\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 58\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 59\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 60\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 61\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 62\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 63\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 64\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 65\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 66\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 67\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 68\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 69\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 70\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 71\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 72\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 73\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | |E]\n",
      "Step: 74\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | |E]\n",
      "Step: 75\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 76\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 77\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 78\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 79\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 80\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 81\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 82\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 83\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 84\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 85\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 86\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 87\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 88\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 89\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 90\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 91\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 92\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 93\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 94\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 95\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 96\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 97\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 98\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 99\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 100\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 101\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 102\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 103\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 104\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 105\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 106\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 107\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 108\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 109\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 110\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 111\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 112\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 113\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 114\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 115\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 116\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 117\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 118\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 119\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 120\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 121\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 122\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 123\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 124\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 125\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 126\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 127\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 128\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 129\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 130\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 131\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 132\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 133\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 134\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 135\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 136\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 137\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 138\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 139\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 140\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 141\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 142\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 143\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 144\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 145\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 146\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 147\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 148\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 149\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 150\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 151\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 152\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 153\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 154\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 155\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 156\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 157\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 158\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 159\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 160\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 161\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 162\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 163\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 164\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 165\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 166\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 167\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 168\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 169\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 170\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 171\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 172\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 173\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 174\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 175\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 176\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 177\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 178\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 179\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 180\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 181\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 182\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 183\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 184\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 185\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 186\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | |E]\n",
      "Step: 187\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 188\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 189\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 190\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 191\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 192\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 193\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 194\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 195\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 196\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 197\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 198\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 199\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 200\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n"
     ]
    }
   ],
   "source": [
    "class SarsaAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.1, discount_factor=0.999, exploration_rate=0.2):\n",
    "        self.q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.prior = None\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        greedy_action_policy = self.q_table.get_greedy_action_policy(state)\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        if state == next_state:\n",
    "            return\n",
    "        if self.prior is not None:\n",
    "            prior_state, prior_action, prior_reward = self.prior\n",
    "            prior_q = self.q_table.table[prior_state][prior_action]\n",
    "            current_q = 0\n",
    "            if state is not None and action is not None:\n",
    "                current_q = self.q_table.table[state][action]\n",
    "            self.q_table.table[prior_state][prior_action] += self.learning_rate * (prior_reward + self.discount_factor * current_q - prior_q)\n",
    "        self.prior = state, action, reward\n",
    "    \n",
    "    def post_episode_learning(self):\n",
    "        self.post_act_learning(None, None, 0, None)\n",
    "        self.prior = None\n",
    "\n",
    "    def print(self):\n",
    "        self.q_table.print()\n",
    "\n",
    "test_agent(SarsaAgent(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Step: 1\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| |E]\n",
      "Step: 8\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | |E]\n",
      "Step: 9\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| |E]\n",
      "Step: 10\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A|E]\n",
      "Step: 11\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n"
     ]
    }
   ],
   "source": [
    "class ExpectedSarsaAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.1, discount_factor=0.999, exploration_rate=0.2):\n",
    "        self.q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        greedy_action_policy = self.q_table.get_greedy_action_policy(state)\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        if state == next_state:\n",
    "            return\n",
    "        current_q = self.q_table.table[state][action]\n",
    "        expected_q = sum(self.q_table.table[next_state]) / len(self.q_table.table[next_state])\n",
    "        self.q_table.table[state][action] += self.learning_rate * (reward + self.discount_factor * expected_q - current_q)\n",
    "\n",
    "    def print(self):\n",
    "        self.q_table.print()\n",
    "\n",
    "test_agent(ExpectedSarsaAgent(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Step: 1\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | |E]\n",
      "Step: 8\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n"
     ]
    }
   ],
   "source": [
    "class StateValueTemporalDifferenceZero(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.2, discount_factor=0.999, exploration_rate=0.2):\n",
    "        self.state_value_table = StateValueTable(environment.get_states_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.environment = environment\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        action_to_next_state_dynamics = environment.get_action_next_states(state)\n",
    "        goal_dynamics = environment.get_exit_states()\n",
    "        greedy_action_policy = self.state_value_table.get_greedy_action_policy(action_to_next_state_dynamics, goal_dynamics)\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, _, reward, next_state):\n",
    "        if state == next_state:\n",
    "            return\n",
    "        next_state_value = self.state_value_table.table[next_state]\n",
    "        current_state = self.state_value_table.table[state]\n",
    "        self.state_value_table.table[state] += self.learning_rate * (reward + self.discount_factor * next_state_value - current_state)\n",
    "\n",
    "    def print(self):\n",
    "        self.state_value_table.print()\n",
    "\n",
    "test_agent(StateValueTemporalDifferenceZero(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Step: 1\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 8\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 9\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 10\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 11\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 12\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 13\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A|E]\n",
      "Step: 14\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n"
     ]
    }
   ],
   "source": [
    "class TemporalDifferenceNStepsToExpectedSarsa(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.2, discount_factor=0.999, exploration_rate=0.2, n_steps=2):\n",
    "        self.q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.n_steps = n_steps\n",
    "        self.priors = []\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        greedy_action_policy = self.q_table.get_greedy_action_policy(state)\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        if state == next_state:\n",
    "            return\n",
    "        self.priors.append((state, action, reward, next_state))\n",
    "        if len(self.priors) >= self.n_steps:\n",
    "            self._update_once_from_priors()\n",
    "\n",
    "    def post_episode_learning(self):\n",
    "        while(self.priors):\n",
    "            self._update_once_from_priors()\n",
    "\n",
    "    def _update_once_from_priors(self):\n",
    "        returns = 0\n",
    "        target_state, target_action, _, _ = self.priors[0]\n",
    "        for step in range(min(self.n_steps, len(self.priors))):\n",
    "            _, _, step_reward, _ = self.priors[step]\n",
    "            returns += step_reward * self.discount_factor ** step\n",
    "        if len(self.priors) == self.n_steps:\n",
    "            _, _, _, last_step_next_state = self.priors[self.n_steps-1]\n",
    "            last_step_expected_q = sum(self.q_table.table[last_step_next_state]) / len(self.q_table.table[last_step_next_state])\n",
    "            returns += last_step_expected_q * self.discount_factor ** self.n_steps\n",
    "        target_current_q = self.q_table.table[target_state][target_action]\n",
    "        self.q_table.table[target_state][target_action] += self.learning_rate * (returns - target_current_q)\n",
    "        self.priors.pop(0)\n",
    "\n",
    "    def print(self):\n",
    "        self.q_table.print()\n",
    "\n",
    "test_agent(TemporalDifferenceNStepsToExpectedSarsa(environment, n_steps=2), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Step: 1\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | |E]\n",
      "Step: 8\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | |E]\n",
      "Step: 9\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "class MonteCarlo(TemporalDifferenceNStepsToExpectedSarsa):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.1, discount_factor=0.999, exploration_rate=0.2):\n",
    "        super().__init__(environment, learning_rate, discount_factor, exploration_rate, n_steps=sys.maxsize)\n",
    "\n",
    "test_agent(MonteCarlo(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BellmansEquations:\n",
    "    @staticmethod\n",
    "    def evaluate_and_improve_policy_iteration(n_states, n_actions, discount, environment_probabilities, environment_rewards, max_evaluate_iterations=100, max_improve_iterations=100, min_evaluation_delta=1e-4):\n",
    "        policy = [0] * n_states\n",
    "        for iteration_index in range(max_improve_iterations):\n",
    "            state_value_function = BellmansEquations._evaluate_policy(n_states, policy, max_evaluate_iterations, discount, min_evaluation_delta, environment_probabilities, environment_rewards)\n",
    "            policy, is_stable = BellmansEquations._improve_policy(n_states, n_actions, policy, state_value_function, discount, environment_probabilities, environment_rewards)\n",
    "            if is_stable:\n",
    "                print(f'Policy improvement stopped at iteration {iteration_index}'); \n",
    "                break\n",
    "        return policy\n",
    "\n",
    "    @staticmethod\n",
    "    def _evaluate_policy(n_states, policy, max_iterations, discount, min_evaluation_delta, environment_probabilities, environment_rewards):\n",
    "        state_value_function = [0] * n_states\n",
    "        for iteration_index in range(max_iterations):\n",
    "            max_delta = 0\n",
    "            for state in range(n_states):\n",
    "                prior_state_value_function = state_value_function[state]\n",
    "                action = policy[state]\n",
    "                state_value_function[state] += BellmansEquations._action_value_update(state, action, state_value_function, environment_probabilities, environment_rewards, discount)\n",
    "                max_delta = max(max_delta, abs(prior_state_value_function-state_value_function[state]))\n",
    "            if max_delta < min_evaluation_delta: \n",
    "                print(f'Policy evaluation stopped at iteration {iteration_index}'); \n",
    "                break\n",
    "        return state_value_function\n",
    "    \n",
    "    @staticmethod\n",
    "    def _improve_policy(n_states, n_actions, policy, state_value_function, discount, environment_probabilities, environment_rewards):\n",
    "        action_value_function = [[0] * n_actions for _ in range(n_states)]\n",
    "        is_stable = True\n",
    "        for state in range(n_states):\n",
    "            for action in range(n_actions):\n",
    "                action_value_function[state][action] = BellmansEquations._action_value_update(state, action, state_value_function, environment_probabilities, environment_rewards, discount)\n",
    "            new_max_actions = PolicyFunctions.get_max_actions(action_value_function[state])\n",
    "            if policy[state] != new_max_actions[0]:\n",
    "                is_stable = False\n",
    "            policy[state] = new_max_actions[0]\n",
    "        return policy, is_stable\n",
    "    \n",
    "    @staticmethod\n",
    "    def policy_value_iteration(n_states, n_actions, discount, environment_probabilities, environment_rewards, max_iterations=100, min_evaluation_delta=1e-4):\n",
    "        state_value_function = [0] * n_states\n",
    "        policy = [0] * n_states\n",
    "        for iteration_index in range(max_iterations):\n",
    "            max_delta = 0\n",
    "            for state in range(n_states):\n",
    "                prior_state_value_function = state_value_function[state]\n",
    "                action_value_function = [\n",
    "                    BellmansEquations._action_value_update(state, action, state_value_function, environment_probabilities, environment_rewards, discount)\n",
    "                    for action in range(n_actions)\n",
    "                ]\n",
    "                state_value_function[state] = max(action_value_function)\n",
    "                max_delta = max(max_delta, abs(prior_state_value_function-state_value_function[state]))\n",
    "                policy[state] = action_value_function.index(max(action_value_function))\n",
    "            if max_delta < min_evaluation_delta: \n",
    "                print(f'Stopped at iteration {iteration_index}'); \n",
    "                break\n",
    "        return policy\n",
    "    \n",
    "    @staticmethod\n",
    "    def _action_value_update(state, action, state_value_function, environment_probabilities, environment_rewards, discount):\n",
    "        return sum([\n",
    "            (next_state_reward_probability * (environment_rewards[reward_index] + discount * state_value_function[next_state]))\n",
    "            for next_state, next_state_reward_probabilities in enumerate(environment_probabilities[state][action])\n",
    "            for reward_index, next_state_reward_probability in enumerate(next_state_reward_probabilities)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation stopped at iteration 0\n",
      "Policy improvement stopped at iteration 8\n",
      "Episode: 1\n",
      "Step: 1\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A|E]\n",
      "Step: 8\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n"
     ]
    }
   ],
   "source": [
    "from DynamicsFunctions import DynamicsFunctions\n",
    "\n",
    "\n",
    "class PolicyIterationAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, discount=0.9):\n",
    "        self.n_actions = environment.get_actions_count()\n",
    "        environment_probabilities = DynamicsFunctions.build_environment_probabilities(environment)\n",
    "        environment_rewards = environment.get_reward_values()\n",
    "        self.policy = BellmansEquations.evaluate_and_improve_policy_iteration(environment.get_states_count(), self.n_actions, discount, environment_probabilities, environment_rewards)\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        policy_action = self.policy[state]\n",
    "        return PolicyFunctions.single_action_as_action_probabilities(policy_action, self.n_actions)\n",
    "\n",
    "    def print(self):\n",
    "        print(self.policy)\n",
    "\n",
    "test_agent(PolicyIterationAgent(environment), environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped at iteration 59\n",
      "Episode: 1\n",
      "Step: 1\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A|E]\n",
      "Step: 8\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n"
     ]
    }
   ],
   "source": [
    "class ValueIterationAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, discount=0.9):\n",
    "        self.n_actions = environment.get_actions_count()\n",
    "        environment_probabilities = DynamicsFunctions.build_environment_probabilities(environment)\n",
    "        environment_rewards = environment.get_reward_values()\n",
    "        self.policy = BellmansEquations.policy_value_iteration(environment.get_states_count(), self.n_actions, discount, environment_probabilities, environment_rewards)\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        policy_action = self.policy[state]\n",
    "        return PolicyFunctions.single_action_as_action_probabilities(policy_action, self.n_actions)\n",
    "\n",
    "    def print(self):\n",
    "        print(self.policy)\n",
    "    \n",
    "test_agent(ValueIterationAgent(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class SoftmaxFunctions:\n",
    "    @staticmethod\n",
    "    def actions_softmax(state_q, temperature):\n",
    "        action_exponentials = [math.exp(q / temperature) for q in state_q]\n",
    "        exponentials_sum = sum(action_exponentials)\n",
    "        return [action_exponential / exponentials_sum for action_exponential in action_exponentials]\n",
    "\n",
    "    @staticmethod\n",
    "    def action_softmax(state_q, action, temperature):\n",
    "        softmaxed_actions = SoftmaxFunctions.actions_softmax(state_q, temperature)\n",
    "        return softmaxed_actions[action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Step: 1\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[A| | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ |A| | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A|E]\n",
      "Step: 8\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n"
     ]
    }
   ],
   "source": [
    "class ReinforceAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, discount=0.9, learning_rate=0.1, softmax_temperature=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.softmax_temperature = softmax_temperature\n",
    "        self.discount = discount\n",
    "        self.critic_state_value_table = StateValueTable(environment.get_states_count())\n",
    "        self.actor_q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.priors = []\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        return SoftmaxFunctions.actions_softmax(self.actor_q_table.table[state], self.softmax_temperature)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        if state == next_state:\n",
    "            return\n",
    "        self.priors.append((state, action, reward, next_state))\n",
    "\n",
    "    def post_episode_learning(self):\n",
    "        step_discount = self.discount ** (len(self.priors)-1)\n",
    "        returns = 0\n",
    "        for step in range(len(self.priors)-1, -1, -1):\n",
    "            state, action, reward, _ = self.priors[step]\n",
    "            returns = reward + step_discount * returns\n",
    "            state_value_delta = returns - self.critic_state_value_table.table[state]\n",
    "            self.critic_state_value_table.table[state] += self.learning_rate * state_value_delta\n",
    "            action_softmax = SoftmaxFunctions.action_softmax(self.actor_q_table.table[state], action, self.softmax_temperature)\n",
    "            self.actor_q_table.table[state][action] += self.learning_rate * state_value_delta * step_discount * (1 - action_softmax) / self.softmax_temperature\n",
    "            step_discount /= self.discount\n",
    "        self.priors = []\n",
    "\n",
    "    def print(self):\n",
    "        self.actor_q_table.print()\n",
    "\n",
    "test_agent(ReinforceAgent(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Step: 1\n",
      "[ | | | | ]\n",
      "[A| | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 2\n",
      "[ | | | | ]\n",
      "[ |A| | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 3\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 4\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | |A| | ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 5\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | | ]\n",
      "[ | | | |E]\n",
      "Step: 6\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | |A| ]\n",
      "[ | | | |E]\n",
      "Step: 7\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n",
      "[ | | | |E]\n",
      "Step: 8\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | | ]\n",
      "[ | | | |A]\n"
     ]
    }
   ],
   "source": [
    "class ActorCriticAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, discount=0.9, learning_rate=0.1, softmax_temperature=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.softmax_temperature = softmax_temperature\n",
    "        self.discount = discount\n",
    "        self.critic_state_value_table = StateValueTable(environment.get_states_count())\n",
    "        self.actor_q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        return SoftmaxFunctions.actions_softmax(self.actor_q_table.table[state], self.softmax_temperature)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        if state == next_state:\n",
    "            return\n",
    "        state_value = self.critic_state_value_table.table[state]\n",
    "        next_state_value = self.critic_state_value_table.table[next_state]\n",
    "        state_value_delta = reward + self.discount * next_state_value - state_value\n",
    "        self.critic_state_value_table.table[state] += self.learning_rate * state_value_delta\n",
    "        action_softmax = SoftmaxFunctions.action_softmax(self.actor_q_table.table[state], action, self.softmax_temperature)\n",
    "        self.actor_q_table.table[state][action] += self.learning_rate * state_value_delta * (1 - action_softmax) / self.softmax_temperature\n",
    "\n",
    "    def print(self):\n",
    "        self.actor_q_table.print()\n",
    "\n",
    "test_agent(ActorCriticAgent(environment), environment)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
