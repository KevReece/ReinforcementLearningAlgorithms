{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.table = [[0] * n_actions for _ in range(n_states)]\n",
    "\n",
    "    def get_greedy_action_policy(self, state):\n",
    "        action_values = self.table[state]\n",
    "        max_q_actions = [\n",
    "            action \n",
    "            for action, q in enumerate(action_values) \n",
    "            if q == max(action_values)\n",
    "        ]\n",
    "        return [\n",
    "            1/len(max_q_actions) if action in max_q_actions else 0 \n",
    "            for action in range(len(action_values))\n",
    "        ]\n",
    "    \n",
    "    def print(self):\n",
    "        for state, actions in enumerate(self.table):\n",
    "            print('State', state, '->', actions)\n",
    "\n",
    "class StateValueTable:\n",
    "    def __init__(self, n_states):\n",
    "        self.table = [0] * n_states\n",
    "\n",
    "    def get_greedy_action_policy(self, action_to_next_state_dynamics, exit_dynamics):\n",
    "        exit_actions = [action for action, next_state in enumerate(action_to_next_state_dynamics) if next_state in exit_dynamics]\n",
    "        greedy_actions = exit_actions\n",
    "        if not greedy_actions:\n",
    "            actions_next_state_values = [self.table[next_state] for next_state in action_to_next_state_dynamics]\n",
    "            max_next_state_value = max([next_state_value for next_state_value in actions_next_state_values])\n",
    "            max_actions = [action for action, next_state_value in enumerate(actions_next_state_values) if next_state_value == max_next_state_value]\n",
    "            greedy_actions = max_actions\n",
    "        return [\n",
    "            1/len(greedy_actions) if action in greedy_actions else 0 \n",
    "            for action in range(len(action_to_next_state_dynamics))\n",
    "        ]\n",
    "\n",
    "    def print(self):\n",
    "        print(self.table)\n",
    "\n",
    "class PolicyFunctions:\n",
    "    @staticmethod\n",
    "    def combine_policy_with_exploratory_policy(action_policy, exploration_rate):\n",
    "        exploratory_action_policy_part = [exploration_rate/len(action_policy)] * len(action_policy)\n",
    "        action_policy_part = [action_policy[action] * (1 - exploration_rate) for action in range(len(action_policy))]\n",
    "        return [exploratory_action_policy_part[action] + action_policy_part[action] for action in range(len(action_policy))]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_actions(state_action_value_function):\n",
    "        max_q = max(state_action_value_function)\n",
    "        max_actions = [action for action, q in enumerate(state_action_value_function) if q == max_q]\n",
    "        return max_actions\n",
    "    \n",
    "    @staticmethod\n",
    "    def single_action_as_action_probabilities(chosen_action, n_actions):\n",
    "        return [\n",
    "            1 if action == chosen_action else 0 \n",
    "            for action in range(n_actions)\n",
    "        ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AbstractAgent import AbstractAgent\n",
    "from Runner import Runner\n",
    "from AbstractEnvironment import AbstractEnvironment\n",
    "from OneDMazeEnvironment import OneDMazeEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent: AbstractAgent, environment: AbstractEnvironment):\n",
    "    environment.reset()\n",
    "    runner = Runner(agent)\n",
    "    runner.run(environment, n_episodes=10000, learn=True)\n",
    "    runner.run(environment, n_episodes=1, verbose=True, learn=False)\n",
    "    agent.print()\n",
    "    \n",
    "# Single goal\n",
    "# environment = OneDMazeEnvironment(initial_agent_position=1, n_states=10, exits=[9], initial_state_rewards={9: 10})\n",
    "    \n",
    "# Duel goal\n",
    "environment = OneDMazeEnvironment(initial_agent_position=3, n_states=10, exits=[0,9], initial_state_rewards={0:1, 9:10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "[E| |A| | | | | | |E]\n",
      "[E|A| | | | | | | |E]\n",
      "[A| | | | | | | | |E]\n",
      "State 0 -> [0, 0]\n",
      "State 1 -> [0.9999999999999996, 0.9980009999999986]\n",
      "State 2 -> [0.998999999999999, 0.997002998999998]\n",
      "State 3 -> [0.9980009999999986, 0.9960059960009975]\n",
      "State 4 -> [0.997002998999998, 0.9950058506766059]\n",
      "State 5 -> [0.9960057309317163, 0.44638237372489803]\n",
      "State 6 -> [0.7651121000444705, 0.005140708877284425]\n",
      "State 7 -> [0.05145854732016441, 0.0]\n",
      "State 8 -> [0, 1.0]\n",
      "State 9 -> [0, 0]\n"
     ]
    }
   ],
   "source": [
    "class QLearningAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.1, discount_factor=0.999, exploration_rate=0.2):\n",
    "        self.q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        greedy_action_policy = self.q_table.get_greedy_action_policy(state)\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        max_next_q = max(self.q_table.table[next_state])\n",
    "        self.q_table.table[state][action] += self.learning_rate * (reward + self.discount_factor * max_next_q - self.q_table.table[state][action])\n",
    "\n",
    "    def print(self):\n",
    "        self.q_table.print()\n",
    "\n",
    "test_agent(QLearningAgent(environment), environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "[E| |A| | | | | | |E]\n",
      "[E|A| | | | | | | |E]\n",
      "[E| |A| | | | | | |E]\n",
      "[E|A| | | | | | | |E]\n",
      "[E| |A| | | | | | |E]\n",
      "[E| | |A| | | | | |E]\n",
      "[E| |A| | | | | | |E]\n",
      "[E| | |A| | | | | |E]\n",
      "[E| |A| | | | | | |E]\n",
      "[E| | |A| | | | | |E]\n",
      "[E| |A| | | | | | |E]\n",
      "[E| | |A| | | | | |E]\n",
      "[E| |A| | | | | | |E]\n",
      "[E|A| | | | | | | |E]\n",
      "[E| |A| | | | | | |E]\n",
      "[E| | |A| | | | | |E]\n",
      "[E| | | |A| | | | |E]\n",
      "[E| | |A| | | | | |E]\n",
      "[E| |A| | | | | | |E]\n",
      "[E|A| | | | | | | |E]\n",
      "[A| | | | | | | | |E]\n",
      "State 0 -> [0, 0]\n",
      "State 1 -> [0.9999999999999996, 9.930209650349724]\n",
      "State 2 -> [9.920279440699368, 9.940149800149882]\n",
      "State 3 -> [9.930209650349724, 9.950099900049942]\n",
      "State 4 -> [9.940149800149882, 9.96005996000996]\n",
      "State 5 -> [9.950099900049942, 9.970029989999968]\n",
      "State 6 -> [9.96005996000996, 9.980009999999975]\n",
      "State 7 -> [9.970029989999968, 9.989999999999984]\n",
      "State 8 -> [9.980009999999975, 9.999999999999993]\n",
      "State 9 -> [0, 0]\n",
      "State 0 -> [0, 0]\n",
      "State 1 -> [0.9999999999999996, 9.930209650349724]\n",
      "State 2 -> [9.920279440699368, 9.940149800149882]\n",
      "State 3 -> [9.930209650349724, 9.950099900049942]\n",
      "State 4 -> [9.940149800149882, 9.96005996000996]\n",
      "State 5 -> [9.950099900049942, 9.970029989999968]\n",
      "State 6 -> [9.96005996000996, 9.980009999999975]\n",
      "State 7 -> [9.970029989999968, 9.989999999999984]\n",
      "State 8 -> [9.980009999999975, 9.999999999999993]\n",
      "State 9 -> [0, 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class DoubleQLearning(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.1, discount_factor=0.999, exploration_rate=0.2):\n",
    "        self.q_table1 = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.q_table2 = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        greedy_action_policy = [\n",
    "            (q1 + q2) / 2 \n",
    "            for q1, q2 in zip(self.q_table1.table[state], self.q_table2.table[state])\n",
    "        ]\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        if random.choice([True, False]):\n",
    "            q_table = self.q_table1.table\n",
    "            source_q_table = self.q_table2.table\n",
    "        else:\n",
    "            q_table = self.q_table2.table\n",
    "            source_q_table = self.q_table1.table\n",
    "        max_action = q_table[next_state].index(max(q_table[next_state]))\n",
    "        q_table[state][action] += self.learning_rate * (reward + self.discount_factor * source_q_table[next_state][max_action] - q_table[state][action])\n",
    "\n",
    "    def print(self):\n",
    "        self.q_table1.print()\n",
    "        self.q_table2.print()\n",
    "\n",
    "test_agent(DoubleQLearning(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "[E| | | |A| | | | |E]\n",
      "[E| | | | |A| | | |E]\n",
      "[E| | | | | |A| | |E]\n",
      "[E| | | | | | |A| |E]\n",
      "[E| | | | | | | |A|E]\n",
      "[E| | | | | | |A| |E]\n",
      "[E| | | | | | | |A|E]\n",
      "[E| | | | | | | | |A]\n",
      "State 0 -> [0, 0]\n",
      "State 1 -> [0.9999999999999996, 7.063417773139303]\n",
      "State 2 -> [3.5804579405352013, 9.914180144280406]\n",
      "State 3 -> [9.333577447504139, 9.933765604925165]\n",
      "State 4 -> [9.824156012520659, 9.950222713707285]\n",
      "State 5 -> [9.922467526249662, 9.960823710545855]\n",
      "State 6 -> [9.948868636582626, 9.973974888678107]\n",
      "State 7 -> [9.961779510140545, 9.988239792874955]\n",
      "State 8 -> [9.97212915535188, 9.999999999999993]\n",
      "State 9 -> [0, 0]\n"
     ]
    }
   ],
   "source": [
    "class SarsaAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.1, discount_factor=0.999, exploration_rate=0.2):\n",
    "        self.q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.prior = None\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        greedy_action_policy = self.q_table.get_greedy_action_policy(state)\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        if self.prior is not None:\n",
    "            prior_state, prior_action, prior_reward = self.prior\n",
    "            prior_q = self.q_table.table[prior_state][prior_action]\n",
    "            current_q = 0\n",
    "            if state is not None and action is not None:\n",
    "                current_q = self.q_table.table[state][action]\n",
    "            self.q_table.table[prior_state][prior_action] += self.learning_rate * (prior_reward + self.discount_factor * current_q - prior_q)\n",
    "        self.prior = state, action, reward\n",
    "    \n",
    "    def post_episode_learning(self):\n",
    "        self.post_act_learning(None, None, 0, None)\n",
    "        self.prior = None\n",
    "\n",
    "    def print(self):\n",
    "        self.q_table.print()\n",
    "\n",
    "test_agent(SarsaAgent(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "[E| |A| | | | | | |E]\n",
      "[E|A| | | | | | | |E]\n",
      "[A| | | | | | | | |E]\n",
      "State 0 -> [0, 0]\n",
      "State 1 -> [0.9999999999999996, 0.6677660748717797]\n",
      "State 2 -> [0.8330212746126426, 0.5043133570450882]\n",
      "State 3 -> [0.6679404553555607, 0.34319521984279094]\n",
      "State 4 -> [0.5043541262314737, 0.18492459115252094]\n",
      "State 5 -> [0.34049698725593247, 0.04502289744945435]\n",
      "State 6 -> [0.15043442834730886, 0.00029280689206155386]\n",
      "State 7 -> [0.012576547180063326, 0.0]\n",
      "State 8 -> [0, 1.0]\n",
      "State 9 -> [0, 0]\n"
     ]
    }
   ],
   "source": [
    "class ExpectedSarsaAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.1, discount_factor=0.999, exploration_rate=0.2):\n",
    "        self.q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        greedy_action_policy = self.q_table.get_greedy_action_policy(state)\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        current_q = self.q_table.table[state][action]\n",
    "        expected_q = sum(self.q_table.table[next_state]) / len(self.q_table.table[next_state])\n",
    "        self.q_table.table[state][action] += self.learning_rate * (reward + self.discount_factor * expected_q - current_q)\n",
    "\n",
    "    def print(self):\n",
    "        self.q_table.print()\n",
    "\n",
    "test_agent(ExpectedSarsaAgent(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "[E| |A| | | | | | |E]\n",
      "[E|A| | | | | | | |E]\n",
      "[A| | | | | | | | |E]\n",
      "[0, 0.9996136198452017, 0.9984869628206533, 0.9970666775103557, 0.9956497263654203, 0.9642809950914902, 0.6838100506051362, 0.4703476832104218, 3.6, 0]\n"
     ]
    }
   ],
   "source": [
    "from OneDMazeDynamics import OneDMazeDynamics\n",
    "\n",
    "\n",
    "class StateValueTemporalDifferenceZero(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.2, discount_factor=0.999, exploration_rate=0.2):\n",
    "        self.state_value_table = StateValueTable(environment.get_states_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.environment = environment\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        action_to_next_state_dynamics = OneDMazeDynamics.get_action_to_next_state_dynamics(state)\n",
    "        goal_dynamics = environment.get_exits()\n",
    "        greedy_action_policy = self.state_value_table.get_greedy_action_policy(action_to_next_state_dynamics, goal_dynamics)\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, _, reward, next_state):\n",
    "        next_state_value = self.state_value_table.table[next_state]\n",
    "        current_state = self.state_value_table.table[state]\n",
    "        self.state_value_table.table[state] += self.learning_rate * (reward + self.discount_factor * next_state_value - current_state)\n",
    "\n",
    "    def print(self):\n",
    "        self.state_value_table.print()\n",
    "\n",
    "test_agent(StateValueTemporalDifferenceZero(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "[E| | | |A| | | | |E]\n",
      "[E| | | | |A| | | |E]\n",
      "[E| | | | | |A| | |E]\n",
      "[E| | | | | | |A| |E]\n",
      "[E| | | | | | | |A|E]\n",
      "[E| | | | | | | | |A]\n",
      "State 0 -> [0, 0]\n",
      "State 1 -> [0.9999149294082696, 8.924814244002501]\n",
      "State 2 -> [9.216081111739111, 9.746494627146994]\n",
      "State 3 -> [8.665104727508094, 9.641825737535338]\n",
      "State 4 -> [9.809185661148655, 9.9040381836926]\n",
      "State 5 -> [9.578223400106046, 9.887812569191041]\n",
      "State 6 -> [9.901347793551393, 9.948053720099756]\n",
      "State 7 -> [9.867391864468159, 9.983755238735613]\n",
      "State 8 -> [9.942824556775816, 9.999999999999996]\n",
      "State 9 -> [0, 0]\n"
     ]
    }
   ],
   "source": [
    "class TemporalDifferenceNStepsToExpectedSarsa(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.2, discount_factor=0.999, exploration_rate=0.2, n_steps=2):\n",
    "        self.q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.n_steps = n_steps\n",
    "        self.priors = []\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        greedy_action_policy = self.q_table.get_greedy_action_policy(state)\n",
    "        return PolicyFunctions.combine_policy_with_exploratory_policy(greedy_action_policy, self.exploration_rate)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        self.priors.append((state, action, reward, next_state))\n",
    "        if len(self.priors) >= self.n_steps:\n",
    "            self._update_once_from_priors()\n",
    "\n",
    "    def post_episode_learning(self):\n",
    "        while(self.priors):\n",
    "            self._update_once_from_priors()\n",
    "\n",
    "    def _update_once_from_priors(self):\n",
    "        returns = 0\n",
    "        target_state, target_action, _, _ = self.priors[0]\n",
    "        for step in range(min(self.n_steps, len(self.priors))):\n",
    "            _, _, step_reward, _ = self.priors[step]\n",
    "            returns += step_reward * self.discount_factor ** step\n",
    "        if len(self.priors) == self.n_steps:\n",
    "            _, _, _, last_step_next_state = self.priors[self.n_steps-1]\n",
    "            last_step_expected_q = sum(self.q_table.table[last_step_next_state]) / len(self.q_table.table[last_step_next_state])\n",
    "            returns += last_step_expected_q * self.discount_factor ** self.n_steps\n",
    "        target_current_q = self.q_table.table[target_state][target_action]\n",
    "        self.q_table.table[target_state][target_action] += self.learning_rate * (returns - target_current_q)\n",
    "        self.priors.pop(0)\n",
    "\n",
    "    def print(self):\n",
    "        self.q_table.print()\n",
    "\n",
    "test_agent(TemporalDifferenceNStepsToExpectedSarsa(environment, n_steps=2), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "[E| |A| | | | | | |E]\n",
      "[E| | |A| | | | | |E]\n",
      "[E| | | |A| | | | |E]\n",
      "[E| | | | |A| | | |E]\n",
      "[E| | | | | |A| | |E]\n",
      "[E| | | | | | |A| |E]\n",
      "[E| | | | | | | |A|E]\n",
      "[E| | | | | | | | |A]\n",
      "State 0 -> [0, 0]\n",
      "State 1 -> [0.9282102012308148, 8.212036801132694]\n",
      "State 2 -> [7.160882110376837, 9.926711130591636]\n",
      "State 3 -> [9.912442269175735, 9.913263225722307]\n",
      "State 4 -> [9.929928635275699, 9.892433178229991]\n",
      "State 5 -> [9.897796226413234, 9.939371634835833]\n",
      "State 6 -> [9.94011899941772, 9.95581485115493]\n",
      "State 7 -> [9.947579338328529, 9.983774534137396]\n",
      "State 8 -> [9.974080562144744, 9.999999999999993]\n",
      "State 9 -> [0, 0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "class MonteCarlo(TemporalDifferenceNStepsToExpectedSarsa):\n",
    "    def __init__(self, environment: AbstractEnvironment, learning_rate=0.1, discount_factor=0.999, exploration_rate=0.2):\n",
    "        super().__init__(environment, learning_rate, discount_factor, exploration_rate, n_steps=sys.maxsize)\n",
    "\n",
    "test_agent(MonteCarlo(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BellmansEquations:\n",
    "    @staticmethod\n",
    "    def evaluate_and_improve_policy_iteration(n_states, n_actions, discount, environment_probabilities, environment_rewards, max_evaluate_iterations=100, max_improve_iterations=100, min_evaluation_delta=1e-4):\n",
    "        policy = [0] * n_states\n",
    "        for iteration_index in range(max_improve_iterations):\n",
    "            state_value_function = BellmansEquations._evaluate_policy(n_states, policy, max_evaluate_iterations, discount, min_evaluation_delta, environment_probabilities, environment_rewards)\n",
    "            policy, is_stable = BellmansEquations._improve_policy(n_states, n_actions, policy, state_value_function, discount, environment_probabilities, environment_rewards)\n",
    "            if is_stable:\n",
    "                print(f'Policy improvement stopped at iteration {iteration_index}'); \n",
    "                break\n",
    "        return policy\n",
    "\n",
    "    @staticmethod\n",
    "    def _evaluate_policy(n_states, policy, max_iterations, discount, min_evaluation_delta, environment_probabilities, environment_rewards):\n",
    "        state_value_function = [0] * n_states\n",
    "        for iteration_index in range(max_iterations):\n",
    "            max_delta = 0\n",
    "            for state in range(n_states):\n",
    "                prior_state_value_function = state_value_function[state]\n",
    "                action = policy[state]\n",
    "                state_value_function[state] += BellmansEquations._action_value_update(state, action, state_value_function, environment_probabilities, environment_rewards, discount)\n",
    "                max_delta = max(max_delta, abs(prior_state_value_function-state_value_function[state]))\n",
    "            if max_delta < min_evaluation_delta: \n",
    "                print(f'Policy evaluation stopped at iteration {iteration_index}'); \n",
    "                break\n",
    "        return state_value_function\n",
    "    \n",
    "    @staticmethod\n",
    "    def _improve_policy(n_states, n_actions, policy, state_value_function, discount, environment_probabilities, environment_rewards):\n",
    "        action_value_function = [[0] * n_actions for _ in range(n_states)]\n",
    "        is_stable = True\n",
    "        for state in range(n_states):\n",
    "            for action in range(n_actions):\n",
    "                action_value_function[state][action] = BellmansEquations._action_value_update(state, action, state_value_function, environment_probabilities, environment_rewards, discount)\n",
    "            new_max_actions = PolicyFunctions.get_max_actions(action_value_function[state])\n",
    "            if policy[state] != new_max_actions[0]:\n",
    "                is_stable = False\n",
    "            policy[state] = new_max_actions[0]\n",
    "        return policy, is_stable\n",
    "    \n",
    "    @staticmethod\n",
    "    def policy_value_iteration(n_states, n_actions, discount, environment_probabilities, environment_rewards, max_iterations=100, min_evaluation_delta=1e-4):\n",
    "        state_value_function = [0] * n_states\n",
    "        policy = [0] * n_states\n",
    "        for iteration_index in range(max_iterations):\n",
    "            max_delta = 0\n",
    "            for state in range(n_states):\n",
    "                prior_state_value_function = state_value_function[state]\n",
    "                action_value_function = [\n",
    "                    BellmansEquations._action_value_update(state, action, state_value_function, environment_probabilities, environment_rewards, discount)\n",
    "                    for action in range(n_actions)\n",
    "                ]\n",
    "                state_value_function[state] = max(action_value_function)\n",
    "                max_delta = max(max_delta, abs(prior_state_value_function-state_value_function[state]))\n",
    "                policy[state] = action_value_function.index(max(action_value_function))\n",
    "            if max_delta < min_evaluation_delta: \n",
    "                print(f'Stopped at iteration {iteration_index}'); \n",
    "                break\n",
    "        return policy\n",
    "    \n",
    "    @staticmethod\n",
    "    def _action_value_update(state, action, state_value_function, environment_probabilities, environment_rewards, discount):\n",
    "        return sum([\n",
    "            (next_state_reward_probability * (environment_rewards[reward_index] + discount * state_value_function[next_state]))\n",
    "            for next_state, next_state_reward_probabilities in enumerate(environment_probabilities[state][action])\n",
    "            for reward_index, next_state_reward_probability in enumerate(next_state_reward_probabilities)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy improvement stopped at iteration 1\n",
      "Episode: 1\n",
      "[E| | | |A| | | | |E]\n",
      "[E| | | | |A| | | |E]\n",
      "[E| | | | | |A| | |E]\n",
      "[E| | | | | | |A| |E]\n",
      "[E| | | | | | | |A|E]\n",
      "[E| | | | | | | | |A]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "class PolicyIterationAgent(AbstractAgent):\n",
    "    def __init__(self, environment: OneDMazeEnvironment, discount=0.9):\n",
    "        self.n_actions = environment.get_actions_count()\n",
    "        environment_probabilities = OneDMazeDynamics.build_environment_probabilities(environment)\n",
    "        environment_rewards = environment.get_reward_values()\n",
    "        self.policy = BellmansEquations.evaluate_and_improve_policy_iteration(environment.get_states_count(), self.n_actions, discount, environment_probabilities, environment_rewards)\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        policy_action = self.policy[state]\n",
    "        return PolicyFunctions.single_action_as_action_probabilities(policy_action, self.n_actions)\n",
    "\n",
    "    def print(self):\n",
    "        print(self.policy)\n",
    "\n",
    "test_agent(PolicyIterationAgent(environment), environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped at iteration 59\n",
      "Episode: 1\n",
      "[E| | | |A| | | | |E]\n",
      "[E| | | | |A| | | |E]\n",
      "[E| | | | | |A| | |E]\n",
      "[E| | | | | | |A| |E]\n",
      "[E| | | | | | | |A|E]\n",
      "[E| | | | | | | | |A]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "class ValueIterationAgent(AbstractAgent):\n",
    "    def __init__(self, environment: OneDMazeEnvironment, discount=0.9):\n",
    "        self.n_actions = environment.get_actions_count()\n",
    "        environment_probabilities = OneDMazeDynamics.build_environment_probabilities(environment)\n",
    "        environment_rewards = environment.get_reward_values()\n",
    "        self.policy = BellmansEquations.policy_value_iteration(environment.get_states_count(), self.n_actions, discount, environment_probabilities, environment_rewards)\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        policy_action = self.policy[state]\n",
    "        return PolicyFunctions.single_action_as_action_probabilities(policy_action, self.n_actions)\n",
    "\n",
    "    def print(self):\n",
    "        print(self.policy)\n",
    "    \n",
    "test_agent(ValueIterationAgent(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class SoftmaxFunctions:\n",
    "    @staticmethod\n",
    "    def actions_softmax(state_q, temperature):\n",
    "        action_exponentials = [math.exp(q / temperature) for q in state_q]\n",
    "        exponentials_sum = sum(action_exponentials)\n",
    "        return [action_exponential / exponentials_sum for action_exponential in action_exponentials]\n",
    "\n",
    "    @staticmethod\n",
    "    def action_softmax(state_q, action, temperature):\n",
    "        softmaxed_actions = SoftmaxFunctions.actions_softmax(state_q, temperature)\n",
    "        return softmaxed_actions[action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "[E| | | |A| | | | |E]\n",
      "[E| | | | |A| | | |E]\n",
      "[E| | | | | |A| | |E]\n",
      "[E| | | | | | |A| |E]\n",
      "[E| | | | | | | |A|E]\n",
      "[E| | | | | | | | |A]\n",
      "State 0 -> [0, 0]\n",
      "State 1 -> [0.7596410486503444, -1.6248582840303691]\n",
      "State 2 -> [0.9691967865042435, -1.1260080418237726]\n",
      "State 3 -> [-5.612780188750774, 1.5796043060647196]\n",
      "State 4 -> [-5.836897658421763, 1.2693089783412013]\n",
      "State 5 -> [-5.262794346677717, 1.34586350573414]\n",
      "State 6 -> [-5.634121571548922, 1.279968737323111]\n",
      "State 7 -> [-5.974904052622924, 1.2470817782988437]\n",
      "State 8 -> [-7.158473275612698, 1.178099171541393]\n",
      "State 9 -> [0, 0]\n"
     ]
    }
   ],
   "source": [
    "class ReinforceAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, discount=0.9, learning_rate=0.1, softmax_temperature=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.softmax_temperature = softmax_temperature\n",
    "        self.discount = discount\n",
    "        self.critic_state_value_table = StateValueTable(environment.get_states_count())\n",
    "        self.actor_q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "        self.priors = []\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        return SoftmaxFunctions.actions_softmax(self.actor_q_table.table[state], self.softmax_temperature)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        self.priors.append((state, action, reward, next_state))\n",
    "\n",
    "    def post_episode_learning(self):\n",
    "        step_discount = self.discount ** (len(self.priors)-1)\n",
    "        returns = 0\n",
    "        for step in range(len(self.priors)-1, -1, -1):\n",
    "            state, action, reward, _ = self.priors[step]\n",
    "            returns = reward + step_discount * returns\n",
    "            state_value_delta = returns - self.critic_state_value_table.table[state]\n",
    "            self.critic_state_value_table.table[state] += self.learning_rate * state_value_delta\n",
    "            action_softmax = SoftmaxFunctions.action_softmax(self.actor_q_table.table[state], action, self.softmax_temperature)\n",
    "            self.actor_q_table.table[state][action] += self.learning_rate * state_value_delta * step_discount * (1 - action_softmax) / self.softmax_temperature\n",
    "            step_discount /= self.discount\n",
    "        self.priors = []\n",
    "\n",
    "    def print(self):\n",
    "        self.actor_q_table.print()\n",
    "\n",
    "test_agent(ReinforceAgent(environment), environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "[E| | | |A| | | | |E]\n",
      "[E| | | | |A| | | |E]\n",
      "[E| | | | | |A| | |E]\n",
      "[E| | | | | | |A| |E]\n",
      "[E| | | | | | | |A|E]\n",
      "[E| | | | | | | | |A]\n",
      "State 0 -> [0, 0]\n",
      "State 1 -> [0.7016086143141989, -0.5748003161339443]\n",
      "State 2 -> [0.1421739831692055, 0.9595776159965699]\n",
      "State 3 -> [-7.045608529465373, 1.5434598751824706]\n",
      "State 4 -> [-5.270055173661396, 1.6417733282204743]\n",
      "State 5 -> [-5.640474119903142, 1.598290386464439]\n",
      "State 6 -> [-5.539051128453065, 1.6172599457150714]\n",
      "State 7 -> [-5.171342295341374, 1.813922982691584]\n",
      "State 8 -> [-5.000259998721385, 2.04695087290606]\n",
      "State 9 -> [0, 0]\n"
     ]
    }
   ],
   "source": [
    "class ActorCriticAgent(AbstractAgent):\n",
    "    def __init__(self, environment: AbstractEnvironment, discount=0.9, learning_rate=0.1, softmax_temperature=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.softmax_temperature = softmax_temperature\n",
    "        self.discount = discount\n",
    "        self.critic_state_value_table = StateValueTable(environment.get_states_count())\n",
    "        self.actor_q_table = QTable(environment.get_states_count(), environment.get_actions_count())\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        return SoftmaxFunctions.actions_softmax(self.actor_q_table.table[state], self.softmax_temperature)\n",
    "\n",
    "    def post_act_learning(self, state, action, reward, next_state):\n",
    "        state_value = self.critic_state_value_table.table[state]\n",
    "        next_state_value = self.critic_state_value_table.table[next_state]\n",
    "        state_value_delta = reward + self.discount * next_state_value - state_value\n",
    "        self.critic_state_value_table.table[state] += self.learning_rate * state_value_delta\n",
    "        action_softmax = SoftmaxFunctions.action_softmax(self.actor_q_table.table[state], action, self.softmax_temperature)\n",
    "        self.actor_q_table.table[state][action] += self.learning_rate * state_value_delta * (1 - action_softmax) / self.softmax_temperature\n",
    "\n",
    "    def print(self):\n",
    "        self.actor_q_table.print()\n",
    "\n",
    "test_agent(ActorCriticAgent(environment), environment)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
