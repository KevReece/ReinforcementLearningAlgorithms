{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from game.game_round import GameRound\n",
    "from game.game_series import GameSeries\n",
    "from agents.human_agent import HumanAgent\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.abstract_agent import AbstractAgent\n",
    "from environment.board import Board\n",
    "from environment.player_type import PlayerType\n",
    "from game.game_reward import GameReward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Alice's turn as X\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |   |        |   |  \n",
      "---------    ---------\n",
      "  | X |        | 1 |  \n",
      "---------    ---------\n",
      "  |   |        |   |  \n",
      "Human Bob's turn as O\n",
      "  |   |        |   |  \n",
      "---------    ---------\n",
      "  | X | O      | 1 | 2\n",
      "---------    ---------\n",
      "  |   |        |   |  \n",
      "Human Alice's turn as X\n",
      "X |   |      3 |   |  \n",
      "---------    ---------\n",
      "  | X | O      | 1 | 2\n",
      "---------    ---------\n",
      "  |   |        |   |  \n",
      "Human Bob's turn as O\n",
      "X |   |      3 |   |  \n",
      "---------    ---------\n",
      "  | X | O      | 1 | 2\n",
      "---------    ---------\n",
      "  | O |        | 4 |  \n",
      "Human Alice's turn as X\n",
      "X |   |      3 |   |  \n",
      "---------    ---------\n",
      "  | X | O      | 1 | 2\n",
      "---------    ---------\n",
      "  | O | X      | 4 | 5\n",
      "Human Alice wins!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GameRound(HumanAgent(\"Alice\"), HumanAgent(\"Bob\")).play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random agent player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Agent Charlie's turn as X\n",
      "  |   |        |   |  \n",
      "---------    ---------\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "  |   |        |   |  \n",
      "Random Agent Damian's turn as O\n",
      "  |   |        |   |  \n",
      "---------    ---------\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "O |   |      2 |   |  \n",
      "Random Agent Charlie's turn as X\n",
      "  |   |        |   |  \n",
      "---------    ---------\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "O |   | X    2 |   | 3\n",
      "Random Agent Damian's turn as O\n",
      "O |   |      4 |   |  \n",
      "---------    ---------\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "O |   | X    2 |   | 3\n",
      "Random Agent Charlie's turn as X\n",
      "O | X |      4 | 5 |  \n",
      "---------    ---------\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "O |   | X    2 |   | 3\n",
      "Random Agent Damian's turn as O\n",
      "O | X |      4 | 5 |  \n",
      "---------    ---------\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "O | O | X    2 | 6 | 3\n",
      "Random Agent Charlie's turn as X\n",
      "O | X |      4 | 5 |  \n",
      "---------    ---------\n",
      "X |   | X    1 |   | 7\n",
      "---------    ---------\n",
      "O | O | X    2 | 6 | 3\n",
      "Random Agent Damian's turn as O\n",
      "O | X |      4 | 5 |  \n",
      "---------    ---------\n",
      "X | O | X    1 | 8 | 7\n",
      "---------    ---------\n",
      "O | O | X    2 | 6 | 3\n",
      "Random Agent Charlie's turn as X\n",
      "O | X | X    4 | 5 | 9\n",
      "---------    ---------\n",
      "X | O | X    1 | 8 | 7\n",
      "---------    ---------\n",
      "O | O | X    2 | 6 | 3\n",
      "Random Agent Charlie wins!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GameRound(RandomAgent(\"Charlie\"), RandomAgent(\"Damian\")).play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versus human player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Alice's turn as X\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "  |   |        |   |  \n",
      "---------    ---------\n",
      "  |   |        |   |  \n",
      "Random Agent Charlie's turn as O\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "  |   |        |   |  \n",
      "---------    ---------\n",
      "O |   |      2 |   |  \n",
      "Human Alice's turn as X\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "  | X |        | 3 |  \n",
      "---------    ---------\n",
      "O |   |      2 |   |  \n",
      "Random Agent Charlie's turn as O\n",
      "X | O |      1 | 4 |  \n",
      "---------    ---------\n",
      "  | X |        | 3 |  \n",
      "---------    ---------\n",
      "O |   |      2 |   |  \n",
      "Human Alice's turn as X\n",
      "X | O |      1 | 4 |  \n",
      "---------    ---------\n",
      "  | X |        | 3 |  \n",
      "---------    ---------\n",
      "O |   | X    2 |   | 5\n",
      "Human Alice wins!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GameRound(HumanAgent(\"Alice\"), RandomAgent(\"Charlie\")).play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning agent player - values iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentModel:\n",
    "    def get_valid_board_state_actions(self, board_state):\n",
    "        valid_state_actions = {}\n",
    "        for action_coords in GridFunctions.all_grid_coordinates(Board.ROWS, Board.COLUMNS):\n",
    "            action_x, action_y = action_coords\n",
    "            if board_state[action_x][action_y] == PlayerType.NO_PLAYER:\n",
    "                valid_state_actions[action_coords] = self._get_resultant_board_state(board_state, action_coords)\n",
    "        return valid_state_actions\n",
    "    \n",
    "    def get_reward_for_board_state(self, board_state):\n",
    "        win_state = self._get_win_state(board_state)\n",
    "        if win_state == None:\n",
    "            return 0\n",
    "        if win_state == PlayerType.NO_PLAYER:\n",
    "            return GameReward.DRAW_REWARD\n",
    "        return GameReward.WIN_REWARD if win_state == PlayerType.PLAYER_1 else GameReward.LOSE_REWARD\n",
    "\n",
    "    def _get_resultant_board_state(self, board_state, action_coords):\n",
    "        resultant_board_state = board_state.copy()\n",
    "        resultant_board_state[action_coords[0]][action_coords[1]] = PlayerType.PLAYER_1\n",
    "        return resultant_board_state    \n",
    "    \n",
    "    def _get_win_state(self, board_state):\n",
    "        for i in range(Board.ROWS):\n",
    "            player_at_intersection = board_state[i][i]\n",
    "            if player_at_intersection == PlayerType.NO_PLAYER:\n",
    "                continue\n",
    "            is_matching_row = board_state[i][0] == board_state[i][1] == board_state[i][2]\n",
    "            is_matching_column = board_state[0][i] == board_state[1][i] == board_state[2][i]\n",
    "            if is_matching_row or is_matching_column:\n",
    "                return player_at_intersection\n",
    "        player_at_centre = board_state[1][1]\n",
    "        if player_at_centre == PlayerType.NO_PLAYER:\n",
    "            return None\n",
    "        is_matching_diagonal = board_state[0][0] == board_state[1][1] == board_state[2][2]\n",
    "        is_matching_inverse_diagonal = board_state[0][2] == board_state[1][1] == board_state[2][0]\n",
    "        if is_matching_diagonal or is_matching_inverse_diagonal:\n",
    "            return player_at_centre\n",
    "        is_board_full = all([x != 0 for row in board_state for x in row])\n",
    "        return PlayerType.NO_PLAYER if is_board_full else None\n",
    "\n",
    "class ValuesIterationPolicy:\n",
    "    def __init__(self, discount=0.9, delta_threshold=0.01, max_iterations=10):\n",
    "        self._discount = discount\n",
    "        self._delta_threshold = delta_threshold\n",
    "        self._state_values = self._initial_states_value()\n",
    "        self._model = EnvironmentModel()\n",
    "        self._iterate_on_values(discount, delta_threshold, max_iterations)\n",
    "\n",
    "    def get_action_policy(self, state):\n",
    "        valid_state_actions = self._model.get_valid_board_state_actions(state)\n",
    "        valid_state_actions_values = {}\n",
    "        for (x, y), resultant_board_state in valid_state_actions.items():\n",
    "            valid_state_actions_values[(x, y)] = self._get_state_value(self._board_state_to_state(resultant_board_state))\n",
    "\n",
    "        actions_values_as_grid = [\n",
    "            [\n",
    "                valid_state_actions_values[(x,y)] if (x,y) in valid_state_actions else 0\n",
    "                for x in range(Board.COLUMNS)\n",
    "            ]\n",
    "            for y in range(Board.ROWS)\n",
    "        ]\n",
    "        return GridFunctions.normalise_grid(actions_values_as_grid, Board.ROWS, Board.COLUMNS)\n",
    "\n",
    "    def handle_reward(self, prior_state, action_taken, new_state, reward):\n",
    "        # this is a dynamic programming model based approach, so learning is upfront\n",
    "        pass\n",
    "\n",
    "    def _iterate_on_values(self, discount, delta_threshold, max_iterations):\n",
    "        for iteration in range(max_iterations):\n",
    "            delta = 0\n",
    "            # iterate over states in reverse order as this matches the reward propagation for this game and for a majority of others\n",
    "            for state in reversed(self._get_all_state_combinations()):\n",
    "                board_state = self._state_to_board_state(state)\n",
    "                current_state_value = self._get_state_value(state)\n",
    "                valid_state_actions = self._model.get_valid_board_state_actions(board_state)\n",
    "                values = []\n",
    "                for _, resultant_board_state in valid_state_actions.items():\n",
    "                    reward = self._model.get_reward_for_board_state(resultant_board_state)\n",
    "                    resultant_state = self._board_state_to_state(resultant_board_state)\n",
    "                    value = reward + discount * self._get_state_value(resultant_state)\n",
    "                    values.append(value)\n",
    "\n",
    "                new_state_value = max(values) if values else 0\n",
    "                self._set_state_value(state, new_state_value)\n",
    "                delta = max(delta, abs(current_state_value - new_state_value))\n",
    "                \n",
    "            print(f\"Iteration: {iteration}, delta: {delta}\")\n",
    "            if delta < delta_threshold:\n",
    "                break\n",
    "\n",
    "    def _initial_states_value(self):\n",
    "        return np.zeros((3, 3, 3, 3, 3, 3, 3, 3, 3))\n",
    "\n",
    "    def _get_state_value(self, state):\n",
    "        return self._state_values[state]\n",
    "    \n",
    "    def _set_state_value(self, state, value):\n",
    "        self._state_values[state] = value\n",
    "\n",
    "    def _get_all_state_combinations(self):\n",
    "        return [\n",
    "            (x0y0, x1y0, x2y0, x0y1, x1y1, x2y1, x0y2, x1y2, x2y2)\n",
    "            for x0y0 in range(3)\n",
    "            for x1y0 in range(3)\n",
    "            for x2y0 in range(3)\n",
    "            for x0y1 in range(3)\n",
    "            for x1y1 in range(3)\n",
    "            for x2y1 in range(3)\n",
    "            for x0y2 in range(3)\n",
    "            for x1y2 in range(3)\n",
    "            for x2y2 in range(3)\n",
    "        ]\n",
    "    \n",
    "    def _state_to_board_state(self, state):\n",
    "        return [\n",
    "            [state[0], state[1], state[2]],\n",
    "            [state[3], state[4], state[5]],\n",
    "            [state[6], state[7], state[8]],\n",
    "        ]\n",
    "    \n",
    "    def _board_state_to_state(self, board_state):\n",
    "        return tuple([item for row in board_state for item in row])\n",
    "\n",
    "class LearningAgent(AbstractAgent):\n",
    "    MAX_ACTIONS = Board.ROWS * Board.COLUMNS\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.policy = ValuesIterationPolicy()\n",
    "        self.round_state_actions = []\n",
    "\n",
    "    def get_move(self, board_plays, current_player):\n",
    "        board_state = self._get_board_state(board_plays, current_player)\n",
    "        action_policy = self._get_state_action_policy(self.policy.get_action_policy(board_state), board_plays)\n",
    "        action_coordinates = self.random_from_grid(action_policy)\n",
    "        self.round_state_actions.append((board_state, action_coordinates))\n",
    "        return action_coordinates\n",
    "\n",
    "    def random_from_grid(self, action_policy):\n",
    "        actions_coordinates = GridFunctions.flatten_to_coordinates(action_policy)\n",
    "        action_index = np.random.choice(range(Board.ROWS*Board.COLUMNS), 1, p=list(actions_coordinates.values())).item()\n",
    "        return list(actions_coordinates.keys())[action_index]\n",
    "\n",
    "    def handle_reward(self, reward, board_plays, current_player):\n",
    "        new_board_state = self._get_board_state(board_plays, current_player)\n",
    "        prior_board_state, action = self.round_state_actions[-1] if self.round_state_actions else (None, None)\n",
    "        self.policy.handle_reward(prior_board_state, action, new_board_state, reward)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Learning Agent {self.name}\"\n",
    "    \n",
    "    def _get_board_state(self, board_plays, current_player):\n",
    "        return self._get_board_plays_in_agent_perspective(board_plays, current_player)\n",
    "\n",
    "    def _get_board_plays_in_agent_perspective(self, board_plays, current_player):\n",
    "        if current_player == PlayerType.PLAYER_1:\n",
    "            # player ordinal matches agent ordinal\n",
    "            return board_plays\n",
    "        player_2_to_agent_a_lookup = {\n",
    "            PlayerType.NO_PLAYER: 0,\n",
    "            PlayerType.PLAYER_2: 1,\n",
    "            PlayerType.PLAYER_1: 2,\n",
    "        }\n",
    "        return [\n",
    "            [player_2_to_agent_a_lookup[square] for square in row]\n",
    "            for row in board_plays \n",
    "        ]\n",
    "    \n",
    "    def _get_state_action_policy(self, action_probabilities, board_plays):\n",
    "        valid_action_probabilities = action_probabilities.copy()\n",
    "        for x, y in GridFunctions.all_grid_coordinates(Board.ROWS, Board.COLUMNS):\n",
    "            if board_plays[x][y] != PlayerType.NO_PLAYER:\n",
    "                valid_action_probabilities[x][y] = 0\n",
    "        return GridFunctions.normalise_grid(valid_action_probabilities, Board.ROWS, Board.COLUMNS)\n",
    "    \n",
    "class GridFunctions:\n",
    "    @staticmethod\n",
    "    def normalise_grid(grid, row_count, column_count):\n",
    "        total = np.sum([grid[x][y] for x, y in GridFunctions.all_grid_coordinates(row_count, column_count)])\n",
    "        if total == 0:\n",
    "            average_value = 1 / (row_count * column_count)\n",
    "            return [[average_value for _ in range(row_count)] for _ in range(column_count)]\n",
    "        return [\n",
    "            [grid[x][y] / total for y in range(row_count)] \n",
    "            for x in range(column_count)\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def all_grid_coordinates(row_count, column_count):\n",
    "        return [(x, y) for x in range(row_count) for y in range(column_count)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def flatten_to_coordinates(grid):\n",
    "        return {\n",
    "            (row_index, column_index): value\n",
    "            for row_index, row in enumerate(grid) \n",
    "            for column_index, value in enumerate(row)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, delta: 3.0\n",
      "Iteration: 1, delta: 0\n",
      "Iteration: 0, delta: 3.0\n",
      "Iteration: 1, delta: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 2, 2, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 2, 2, 2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_agent_a = LearningAgent(\"Emma\")\n",
    "learning_agent_b = LearningAgent(\"Fred\")\n",
    "GameSeries(20, learning_agent_a, learning_agent_b).play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versus random agent player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 0, 0, 1, 1, 2, 1, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GameSeries(20, learning_agent_a, RandomAgent(\"Charlie\")).play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versus human player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Agent Emma's turn as X\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "  |   |        |   |  \n",
      "---------    ---------\n",
      "  |   |        |   |  \n",
      "Human Alice's turn as O\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "  | O |        | 2 |  \n",
      "---------    ---------\n",
      "  |   |        |   |  \n",
      "Learning Agent Emma's turn as X\n",
      "X |   |      1 |   |  \n",
      "---------    ---------\n",
      "  | O |        | 2 |  \n",
      "---------    ---------\n",
      "  |   | X      |   | 3\n",
      "Human Alice's turn as O\n",
      "X |   | O    1 |   | 4\n",
      "---------    ---------\n",
      "  | O |        | 2 |  \n",
      "---------    ---------\n",
      "  |   | X      |   | 3\n",
      "Learning Agent Emma's turn as X\n",
      "X |   | O    1 |   | 4\n",
      "---------    ---------\n",
      "  | O | X      | 2 | 5\n",
      "---------    ---------\n",
      "  |   | X      |   | 3\n",
      "Human Alice's turn as O\n",
      "X |   | O    1 |   | 4\n",
      "---------    ---------\n",
      "  | O | X      | 2 | 5\n",
      "---------    ---------\n",
      "O |   | X    6 |   | 3\n",
      "Human Alice wins!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GameRound(learning_agent_a, HumanAgent(\"Alice\")).play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
